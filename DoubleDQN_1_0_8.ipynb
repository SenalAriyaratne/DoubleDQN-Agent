{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSJUgL7T-Vk4",
        "outputId": "27a141ef-4259-42bb-94db-9a6559e6dfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Installing collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376133 sha256=99d95c27981d1472a9a65000f78aec1b7a358ed1cb9175800301289918593bbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install torch\n",
        "!pip install torchinfo\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JgP6PsMwSQSo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torchinfo import summary\n",
        "import gymnasium as gym\n",
        "from gymnasium.spaces import Discrete, Box\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import abc\n",
        "from abc import ABC, abstractmethod\n",
        "import typing\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from itertools import count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ju9-8ORQMsz5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CircularQueue():\n",
        "    \"\"\"\n",
        "    A Circular Queue implementation using Python.\n",
        "\n",
        "    Attributes:\n",
        "        __max_size (int): The maximum size of the queue.\n",
        "        __queue (numpy.ndarray): The array to store queue elements.\n",
        "        __head_idx (int): The index of the front element.\n",
        "        __tail_idx (int): The index of the rear element.\n",
        "        __size (int): The current size of the queue.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_size):\n",
        "        \"\"\"\n",
        "        Initializes a CircularQueue with the specified maximum size.\n",
        "\n",
        "        Args:\n",
        "            max_size (int): The maximum size of the queue.\n",
        "        \"\"\"\n",
        "        self.__max_size = max_size\n",
        "        self.__queue = np.empty(max_size, dtype=np.ndarray)\n",
        "        self.__head_idx = -1\n",
        "        self.__tail_idx = -1\n",
        "        self.__size = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the current size of the queue.\n",
        "        \"\"\"\n",
        "        return self.__size\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Returns a string representation of the queue.\n",
        "        \"\"\"\n",
        "        return str(self.__queue)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the element at the specified index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the element to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            object: The element at the specified index.\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the index is out of range.\n",
        "        \"\"\"\n",
        "        if idx < 0 or idx >= self.__size:\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        return self.__queue[(self.__head_idx + idx) % self.__max_size]\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Returns an iterator for the queue.\n",
        "        \"\"\"\n",
        "        self.__iter_idx = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Returns the next element in the queue.\n",
        "        \"\"\"\n",
        "        if self.__iter_idx < self.__size:\n",
        "            item = self.__queue[(self.__head_idx + self.__iter_idx) % self.__max_size]\n",
        "            self.__iter_idx += 1\n",
        "            return item\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "    def is_empty(self) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the queue is empty.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the queue is empty, False otherwise.\n",
        "        \"\"\"\n",
        "        return self.__size == 0\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the queue is empty.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the queue is empty, False otherwise.\n",
        "        \"\"\"\n",
        "        if self.__head_idx == 0  and self.__tail_idx == self.__max_size - 1:\n",
        "          return True\n",
        "        if self.__head_idx == self.__tail_idx + 1:\n",
        "          return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "    def enqueue(self, item) -> bool:\n",
        "        \"\"\"\n",
        "        Adds an element to the rear of the queue.\n",
        "\n",
        "        Args:\n",
        "            item (object): The element to be added.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the operation is successful, False otherwise.\n",
        "        \"\"\"\n",
        "        if self.is_full():\n",
        "          return False\n",
        "        if self.is_empty():\n",
        "          self.__head_idx = 0\n",
        "\n",
        "        self.__tail_idx = (self.__tail_idx + 1) % self.__max_size\n",
        "        self.__queue[self.__tail_idx] = item\n",
        "        self.__size += 1\n",
        "        return True\n",
        "\n",
        "    def dequeue(self) -> bool:\n",
        "        \"\"\"\n",
        "        Removes an element from the front of the queue.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the operation is successful, False otherwise.\n",
        "        \"\"\"\n",
        "        if self.is_empty(): # Return from the method if the queue is empty\n",
        "          return False\n",
        "        elif(self.__head_idx == self.__tail_idx): # Reset front and rear indices if the front index equals the rear index\n",
        "          self.__queue[self.__head_idx] = None\n",
        "          self.__head_idx = -1\n",
        "          self.__tail_idx = -1\n",
        "          self.__size -= 1\n",
        "          return True\n",
        "        else: # Circularly increment the head index by one: % operation is utilised here to create a circular behaviour.\n",
        "          self.__queue[self.__head_idx] = None\n",
        "          self.__head_idx = (self.__head_idx + 1) % self.__max_size\n",
        "          self.__size -= 1\n",
        "          return True\n",
        "\n",
        "    def return_queue(self):\n",
        "        \"\"\"\n",
        "        Returns the underlying array representation of the queue.\n",
        "        \"\"\"\n",
        "        return self.__queue\n",
        "\n",
        "    def show_queue(self):\n",
        "        \"\"\"Prints the current elements in the queue.\"\"\"\n",
        "        print(self.__queue)\n",
        "\n",
        "    def peek(self):\n",
        "        \"\"\"\n",
        "        Returns the element at the front of the queue without removing it.\n",
        "        \"\"\"\n",
        "        if self.is_empty():\n",
        "            print(\"Queue is empty\")\n",
        "            return None\n",
        "        else:\n",
        "            return self.__queue[self.__head_idx]\n",
        "\n",
        "    def get_head_idx(self):\n",
        "        \"\"\"\n",
        "        Returns the index of the front element.\n",
        "\n",
        "        Returns:\n",
        "            int: The index of the front element.\n",
        "        \"\"\"\n",
        "        return int(self.__head_idx)\n",
        "\n",
        "    def get_tail_idx(self):\n",
        "        \"\"\"\n",
        "        Returns the index of the rear element.\n",
        "\n",
        "        Returns:\n",
        "            int: The index of the rear element.\n",
        "        \"\"\"\n",
        "        return int(self.__tail_idx)\n",
        "\n",
        "    def clear_queue(self):\n",
        "        \"\"\"Removes all elements from the queue.\"\"\"\n",
        "        #self.__queue = np.empty(self.__max_size, dtype=np.ndarray)\n",
        "        for i in range(self.__max_size):\n",
        "          if self.__queue[i] == None:\n",
        "            continue\n",
        "          else:\n",
        "            self.__queue[i] = None\n",
        "        self.__head_idx = -1\n",
        "        self.__tail_idx = -1\n",
        "        self.__size = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k_ChHbVQUWD2"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay(ABC):\n",
        "  \"\"\"\n",
        "    ExperienceReplay is an abstract base class representing the interface for in memory experience replay buffers\n",
        "    used in reinforcement learning scenarios. Subclasses must implement the push and sample methods.\n",
        "\n",
        "    Attributes:\n",
        "    - max_size (int): The maximum size of the experience replay buffer.\n",
        "\n",
        "    Methods:\n",
        "    - __init__(self, max_size): Constructor method to initialize the maximum size of the replay buffer.\n",
        "\n",
        "    - push(self, experience: AgentExperience): Abstract method to add an experience to the replay buffer.\n",
        "\n",
        "    - sample(self, batch_size: int): Abstract method to randomly sample a batch of experiences from the replay buffer.\n",
        "\n",
        "    Usage Example:\n",
        "    >>> class MyReplayBuffer(ExperienceReplay):\n",
        "    ...     def __init__(self, max_size):\n",
        "    ...         super(MyReplayBuffer, self).__init__(max_size)\n",
        "    ...\n",
        "    ...     def push(self, experience: AgentExperience):\n",
        "    ...         # Implementation of push method\n",
        "    ...         pass\n",
        "    ...\n",
        "    ...     def sample(self, batch_size: int):\n",
        "    ...         # Implementation of sample method\n",
        "    ...         pass\n",
        "\n",
        "    Note: Subclasses of ExperienceReplay must provide concrete implementations for the push and sample methods.\n",
        "    The sample method can return any data structure (e.g., list, queue, etc.) based on the specific implementation.\n",
        "    \"\"\"\n",
        "  def __init__(self, max_size):\n",
        "    \"\"\"\n",
        "        Constructor method to initialize the maximum size of the experience replay buffer.\n",
        "\n",
        "        Args:\n",
        "        - max_size (int): The maximum size of the experience replay buffer.\n",
        "    \"\"\"\n",
        "    self.max_size = max_size\n",
        "\n",
        "  @abstractmethod\n",
        "  def push(self, experience: Tuple):\n",
        "    \"\"\"\n",
        "        Abstract method to add an experience to the replay buffer.\n",
        "\n",
        "        Args:\n",
        "        - experience (Tuple): The experience to be added to the replay buffer.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def return_buffer(self):\n",
        "    pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hbI5Pb-fTX-V"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ReplayBuffer(ExperienceReplay):\n",
        "    \"\"\"\n",
        "    ReplayBuffer class represents a replay buffer for storing and sampling experiences in a reinforcement learning scenario.\n",
        "    It utilizes a circular queue to store experiences and allows random sampling for training purposes.\n",
        "\n",
        "    Attributes:\n",
        "    - max_size (int): The maximum size of the replay buffer.\n",
        "    - circular_queue (DoubleEndedQueue): The circular queue used to store experiences.\n",
        "\n",
        "    Methods:\n",
        "    - __init__(self, max_size): Constructor method to initialize the ReplayBuffer.\n",
        "    - push(self, experience): Adds an experience to the replay buffer, replacing the oldest if the buffer is full.\n",
        "    - __len__(self): Returns the current number of experiences in the replay buffer.\n",
        "    - sample(self, batch_size): Randomly samples a batch of experiences from the replay buffer.\n",
        "    - __repr__(self): Returns a string representation of the ReplayBuffer instance.\n",
        "\n",
        "    Usage Example:\n",
        "    >>> buffer = ReplayBuffer(max_size=100)\n",
        "    >>> experience1 = AgentExperience(time_stamp=1, state=[1, 2, 3], action='move', next_state=[2, 3, 4], reward=1.0)\n",
        "    >>> buffer.push(experience1)\n",
        "    >>> len(buffer)\n",
        "    1\n",
        "    >>> experience2 = AgentExperience(time_stamp=2, state=[2, 3, 4], action='jump', next_state=[3, 4, 5], reward=0.5)\n",
        "    >>> buffer.push(experience2)\n",
        "    >>> sampled_batch = buffer.sample(batch_size=32)\n",
        "    >>> print(sampled_batch)\n",
        "\n",
        "    Note: This class is designed to be used as a replay buffer for experience storage and sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_size):\n",
        "        \"\"\"\n",
        "        Initializes a ReplayBuffer with the specified maximum size.\n",
        "\n",
        "        Args:\n",
        "        - max_size (int): The maximum size of the replay buffer.\n",
        "        \"\"\"\n",
        "        super().__init__(max_size)\n",
        "        self.max_size = max_size\n",
        "        self.circular_queue = CircularQueue(max_size)\n",
        "\n",
        "    def push(self, experience: Any):\n",
        "        \"\"\"\n",
        "        Adds an experience to the replay buffer, replacing the oldest if the buffer is full.\n",
        "\n",
        "        Args:\n",
        "        - experience (AgentExperience): The experience to be added to the replay buffer.\n",
        "        \"\"\"\n",
        "        if self.circular_queue.is_full():\n",
        "            self.circular_queue.dequeue()\n",
        "        self.circular_queue.enqueue(experience)\n",
        "\n",
        "    def return_buffer(self):\n",
        "      return self.circular_queue.return_queue()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the current number of experiences in the replay buffer.\n",
        "\n",
        "        Returns:\n",
        "        - int: The number of experiences in the replay buffer.\n",
        "        \"\"\"\n",
        "        return len(self.circular_queue)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Returns a string representation of the ReplayBuffer instance.\n",
        "\n",
        "        Returns:\n",
        "        - str: A string representation of the ReplayBuffer instance.\n",
        "        \"\"\"\n",
        "        return f\"ReplayBuffer({str(self.circular_queue)})\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      return self.circular_queue[index]\n",
        "\n",
        "    def __iter__(self):\n",
        "      self.iter_idx = 0\n",
        "      return self\n",
        "\n",
        "    def __next__(self) -> Any:\n",
        "      if self.iter_idx < len(self.circular_queue):\n",
        "        item = self.circular_queue[self.iter_idx]\n",
        "        self.iter_idx += 1\n",
        "        return item\n",
        "      else:\n",
        "        raise StopIteration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dOMqAwxGTyV6"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "  \"\"\"\n",
        "    DQN (Deep Q-Network) class represents a simple feedforward neural network for reinforcement learning.\n",
        "    It consists of three fully connected layers with ReLU activation functions.\n",
        "\n",
        "    Attributes:\n",
        "    - input_dims (int): The number of input dimensions for the neural network.\n",
        "    - num_actions (int): The number of output actions the network can predict.\n",
        "    - hidden_units (int): The number of units in the hidden layers of the network.\n",
        "    - fc_layer_one (nn.Linear): The first fully connected layer.\n",
        "    - fc_layer_two (nn.Linear): The second fully connected layer.\n",
        "    - fc_layer_three (nn.Linear): The third fully connected layer.\n",
        "\n",
        "    Methods:\n",
        "    - __init__(self, input_dims, num_actions, hidden_units): Constructor method to initialize the DQN.\n",
        "    - forward(self, x): Forward method to define the forward pass of the neural network.\n",
        "\n",
        "    Usage Example:\n",
        "    >>> dqn = DQN(input_dims=64, num_actions=4, hidden_units=128)\n",
        "    >>> input_tensor = torch.randn((1, 64))  # Input tensor with shape (batch_size, input_dims)\n",
        "    >>> output_tensor = dqn(input_tensor)  # Output tensor with shape (batch_size, num_actions)\n",
        "\n",
        "    Note: This class extends nn.Module and uses PyTorch's neural network modules for implementation.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_dims, num_actions, hidden_units):\n",
        "    \"\"\"\n",
        "        Initializes a DQN with the specified input dimensions, number of actions, and hidden units.\n",
        "\n",
        "        Args:\n",
        "        - input_dims (int): The number of input dimensions for the neural network.\n",
        "        - num_actions (int): The number of output actions the network can predict.\n",
        "        - hidden_units (int): The number of units in the hidden layers of the network.\n",
        "    \"\"\"\n",
        "    super(DQN, self).__init__()\n",
        "    self.__hidden_units = hidden_units\n",
        "    self.input_dims = input_dims\n",
        "    self.num_actions = num_actions\n",
        "    self.fc_layer_one = nn.Linear(in_features=self.input_dims, out_features=self.__hidden_units)\n",
        "    self.fc_layer_two = nn.Linear(in_features=self.__hidden_units, out_features=self.__hidden_units)\n",
        "    self.fc_layer_three = nn.Linear(in_features=self.__hidden_units, out_features=self.num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "        Defines the forward pass of the neural network.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): The input tensor to the neural network.\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: The output tensor after the forward pass.\n",
        "    \"\"\"\n",
        "    x = F.relu(self.fc_layer_one(x))\n",
        "    x = F.relu(self.fc_layer_two(x))\n",
        "    x = self.fc_layer_three(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BtF6RenzgCPU"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkFactory:\n",
        "\n",
        "  @staticmethod\n",
        "  def create_neural_network(neural_network_type:str, input_dims, num_actions:int, hidden_units:int):\n",
        "    \"\"\"\n",
        "        Creates a neural network of the specified type.\n",
        "\n",
        "        Args:\n",
        "        - neural_network_type (str): The type of neural network to create.\n",
        "        - input_dims (int): The number of input dimensions for the neural network.\n",
        "        - num_actions (int): The number of output actions the network can predict.\n",
        "        - hidden_units (int): The number of units in the hidden layers of the network.\n",
        "\n",
        "        Returns:\n",
        "        - torch.nn.Module: The neural network object.\n",
        "        - neural network type: The type of neural network created\n",
        "    \"\"\"\n",
        "    if neural_network_type == \"online\":\n",
        "      return DQN(input_dims, num_actions, hidden_units), neural_network_type\n",
        "    elif neural_network_type == \"target\":\n",
        "      return DQN(input_dims, num_actions, hidden_units) , neural_network_type\n",
        "    else:\n",
        "      raise ValueError(\"Invalid neural network type: {}\".format(neural_network_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "62ZCLfIDQhxL"
      },
      "outputs": [],
      "source": [
        "class ModelSpecs:\n",
        "  \"\"\"\n",
        "    ModelSpecs class provides utility methods for summarizing and inspecting PyTorch neural network models.\n",
        "\n",
        "    Attributes:\n",
        "    - neural_net (torch.nn.Module): The neural network model to be analyzed.\n",
        "\n",
        "    Methods:\n",
        "    - __init__(self, neural_net): Constructor method to initialize ModelSpecs with a neural network.\n",
        "    - show_summary(self, input_size): Displays a summary of the model, including the number of parameters and input/output sizes.\n",
        "    - _print_input_output_size(self, input_size): Helper method to print input and output sizes of the model.\n",
        "    - show_parameters(self): Displays the parameters of the model.\n",
        "    - show_gradients(self): Displays the gradients of the model parameters.\n",
        "    - show_layer_names(self): Displays the names of the layers in the model.\n",
        "\n",
        "    Usage Example:\n",
        "    >>> model = Initialize pytorch model\n",
        "    >>> specs = ModelSpecs(model)\n",
        "    >>> specs.show_summary(input_size=(1, 64))\n",
        "    >>> specs.show_parameters()\n",
        "    >>> specs.show_gradients()\n",
        "    >>> specs.show_layer_names()\n",
        "\n",
        "    Note: This class is designed to provide convenient methods for inspecting and summarizing PyTorch neural network models.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, neural_net):\n",
        "    \"\"\"\n",
        "        Initializes ModelSpecs with the specified neural network model.\n",
        "\n",
        "        Args:\n",
        "        - neural_net (torch.nn.Module): The neural network model to be analyzed.\n",
        "    \"\"\"\n",
        "    self.neural_net = neural_net\n",
        "\n",
        "  def show_summary(self, input_size):\n",
        "     \"\"\"\n",
        "        Displays a summary of the model, including the number of parameters and input/output sizes.\n",
        "\n",
        "        Args:\n",
        "        - input_size (tuple): The size of the input tensor (e.g., (batch_size, input_dims)).\n",
        "     \"\"\"\n",
        "     print(\"Model Summary:\")\n",
        "     print(self.neural_net)\n",
        "     print(\"\\nNumber of parameters:\", sum(p.numel() for p in self.neural_net.parameters()))\n",
        "     self.__print_input_output_size(input_size)\n",
        "\n",
        "  def __print_input_output_size(self, input_size):\n",
        "      \"\"\"\n",
        "        Helper method to print input and output sizes of the model.\n",
        "\n",
        "        Args:\n",
        "        - input_size (tuple): The size of the input tensor (e.g., (batch_size, input_dims)).\n",
        "      \"\"\"\n",
        "      # * operator is used to unpack the elments if the input is a tuple..\n",
        "      #it passes each element of the tuple as a seperate argument to the function\n",
        "      dummy_input = torch.randn(*input_size)\n",
        "      output = self.neural_net(dummy_input)\n",
        "      print(\"\\nInput Size:\", input_size)\n",
        "      print(\"Output Size:\", output.size())\n",
        "\n",
        "  def show_parameters(self):\n",
        "    \"\"\"Displays the parameters of the model.\"\"\"\n",
        "    print(\"Model Parameters:\")\n",
        "    for name, param in self.neural_net.named_parameters():\n",
        "      print(name, param.size())\n",
        "\n",
        "  def show_gradients(self):\n",
        "    \"\"\"Displays the gradients of the model parameters.\"\"\"\n",
        "    print(\"Model Gradients:\")\n",
        "    for name, param in self.neural_net.named_parameters():\n",
        "      print(name, param.grad)\n",
        "\n",
        "  def show_layer_names(self):\n",
        "    \"\"\"Displays the names of the layers in the model.\"\"\"\n",
        "    print(\"Model Layer Names:\")\n",
        "    for name, layer in self.neural_net.named_children():\n",
        "      print(name, layer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessorDiscrete:\n",
        "\n",
        "  def __init__(self, state_dimensions: int):\n",
        "    self.__batch = None\n",
        "    self.__dims = state_dimensions\n",
        "\n",
        "  def batch_to_processe(self, batch:np.ndarray):\n",
        "    self.__batch = batch\n",
        "\n",
        "  def filter_to_array(self, filter_index: int) -> np.ndarray:\n",
        "    filter_array = np.array([item[filter_index] for item in self.__batch if item is not None], dtype=np.float32)\n",
        "    return filter_array\n",
        "\n",
        "  def create_masks(self, check_array: np.ndarray , check_value: int, is_final: bool) -> np.ndarray:\n",
        "    if is_final == True:\n",
        "      mask_array = np.all(check_array == check_value, axis=1)\n",
        "    else:\n",
        "      mask_array = np.all(check_array != check_value, axis=1)\n",
        "    return mask_array\n",
        "\n",
        "  def array_info(self, check_array: np.ndarray):\n",
        "    print(f\"Array Information\")\n",
        "    print(f\"---------------------------------\")\n",
        "    print(f\"Shape - {check_array.shape}\")\n",
        "    print(f\"Type  - {type(check_array)}\")"
      ],
      "metadata": {
        "id": "FnlCijT2Np_f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler:\n",
        "\n",
        "  def __init__(self, batch_size: int):\n",
        "    self.__raw_experinces = None\n",
        "    self.__curent_size = None\n",
        "    self.__batch_size = batch_size\n",
        "    self.__aux_buffer = None\n",
        "\n",
        "  def get_expereinces(self, experiences: ReplayBuffer):\n",
        "    self.__raw_experinces = experiences\n",
        "    self.__current_size = len(experiences)\n",
        "\n",
        "  def create_valid_buffer(self):\n",
        "    self.__aux_buffer = ReplayBuffer(max_size=self.__current_size)\n",
        "\n",
        "  def fill_buffer(self):\n",
        "    for i , item in enumerate(self.__raw_experinces):\n",
        "      if item is not None:\n",
        "        self.__aux_buffer.push(item)\n",
        "      else:\n",
        "        print(f\"None value found at. {i}\")\n",
        "\n",
        "  def get_buffer(self) -> ReplayBuffer:\n",
        "    return self.__aux_buffer\n",
        "\n",
        "  def sample_batch(self):\n",
        "    queue = self.__aux_buffer.return_buffer()\n",
        "    sample_batch = np.random.choice(queue,self.__batch_size)\n",
        "    return sample_batch"
      ],
      "metadata": {
        "id": "wXt8WhTQZcRs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HYP3_fKEsN"
      },
      "source": [
        " # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Check for Valid expereinces in the replay buffer; Values that are not None***\n",
        "\n"
      ],
      "metadata": {
        "id": "NbQJkZyHVdDv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cr62p1lMT4Xt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "54f74359-339a-4c64-f608-cb3909051843"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-19e6cd012b35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDoubleDQNAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-19e6cd012b35>\u001b[0m in \u001b[0;36mDoubleDQNAgent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDoubleDQNAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
          ]
        }
      ],
      "source": [
        "class DoubleDQNAgent:\n",
        "\n",
        "  def __init__(self, hyper_params: Dict, env_name: str, **kwargs):\n",
        "\n",
        "\n",
        "    #get necessary parameters from environment\n",
        "    self.env = gym.make(env_name, **kwargs)\n",
        "    self.__num_actions = self.env.action_space.n\n",
        "    self.__input_state_dim = self.env.observation_space.shape[0]\n",
        "    self.__device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "    #DDQN param\n",
        "    self.__paramters = hyper_params\n",
        "    self.__hidden_units = self.__paramters[\"hidden_units\"]\n",
        "    self.__discount_factor = self.__paramters[\"discount_factor\"]\n",
        "    self.__learning_rate = self.__paramters[\"learning_rate\"]\n",
        "    self.__training_epochs = self.__paramters[\"training_epochs\"]#number episodes\n",
        "    self.__memory_capacity = self.__paramters[\"memory_capacity\"] # max buffer suze 2000\n",
        "    self.__epsilon_max = self.__paramters[\"epsilon_max\"]\n",
        "    self.__epsilon_decay = self.__paramters[\"epsilon_decay\"] # variable that governs the behavior of downgrading epsilon\n",
        "    self.__epsilon_min = self.__paramters[\"epsilon_min\"]\n",
        "    self.__batch_size = self.__paramters[\"batch_size\"] # 32,64,128 start with 32\n",
        "    self.__update_target_network_freq = self.__paramters[\"update_target_network_freq\"] # every 10 epsiodes if max episodes 1000\n",
        "    self.__evaluate_model_freq = self.__paramters[\"evaluate_model_freq\"]# every 100 episodes if max episodes is 1000 (experiemnt with these numbers)\n",
        "    self.__optimise_model_freq = self.__paramters[\"optimise_model_freq\"] #4 steps or 10 steps refer to timesteps\n",
        "    self.__model_evaluate_duration = 100 # 100 episodes\n",
        "\n",
        "    self.__loss_function = self.__paramters[\"loss_function\"]\n",
        "    #Build experience replay buffer\n",
        "    self.__replay_memory = ReplayBuffer(max_size=self.__memory_capacity)\n",
        "    self.__mini_batch = None\n",
        "    #Build online and target networks\n",
        "    self.__online_network, _ = NeuralNetworkFactory.create_neural_network(neural_network_type=\"online\",\n",
        "                                                                       input_dims= self.__input_state_dim,\n",
        "                                                                       num_actions=self.__num_actions,\n",
        "                                                                       hidden_units=self.__hidden_units)\n",
        "    self.__target_network, _ = NeuralNetworkFactory.create_neural_network(neural_network_type=\"target\",\n",
        "                                                                       input_dims= self.__input_state_dim,\n",
        "                                                                       num_actions=self.__num_actions,\n",
        "                                                                       hidden_units=self.__hidden_units)\n",
        "    self.__optimizer = optim.AdamW(self.__online_network.parameters(), lr=self.__learning_rate, amsgrad=True)\n",
        "    #Data processing and sampling\n",
        "    self.__data_processor = DataProcessorDiscrete(self.__input_state_dim)\n",
        "    self.__sampler = Sampler(batch_size=self.__batch_size)\n",
        "\n",
        "    #Model Summary\n",
        "\n",
        "    #agent performance metrics\n",
        "    self.__moving_average_reward_list = []\n",
        "    self.__episode_reward_list = []\n",
        "    self.__best_mean_rewards_list = []\n",
        "\n",
        "\n",
        "    #agent evaluation\n",
        "    self.__current_best_policy = None\n",
        "    self.__current_best_mean_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "  def train_agent(self):\n",
        "    episode_reward_list = []\n",
        "    #take the average of 10 or 100 epsiodes and add to the list\n",
        "    moving_average_reward_list = []\n",
        "    mean_evaluate_reward_list = []\n",
        "    model_optimise_counter = 0\n",
        "    model_evaluate_counter = 0\n",
        "\n",
        "\n",
        "    for episode in range(self.__training_epochs):\n",
        "      episode_steps = 0\n",
        "      episode_reward = 0\n",
        "      if episode == 0:\n",
        "        self.__epsilon = self.__epsilon_max\n",
        "      else:\n",
        "        self.__epsilon = self.__shrink_epsilon(self.__epsilon_min, self.__epsilon_max, self.__epsilon_decay, episode)\n",
        "      print(f\"Starting Episode No : {episode} | Epsilon : {self.__epsilon} ******************************************************************\")\n",
        "      #reset environment\n",
        "      state, _ = self.env.reset()\n",
        "      state_tensor = torch.tensor(state, device=self.__device).unsqueeze(0)\n",
        "\n",
        "      for time_step in count():\n",
        "        episode_steps += 1\n",
        "        action = self.__select_action(self.__epsilon, state_tensor)\n",
        "        next_state, reward, termi, _, _ = self.env.step(action.item())\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if termi :\n",
        "          next_state = -1 * np.ones(self.__input_state_dim, dtype=np.ndarray)\n",
        "          agent_experience = (state, next_state, action.item(), reward)\n",
        "          self.__replay_memory.push(agent_experience)\n",
        "          break\n",
        "        else:\n",
        "          agent_experience = (state, next_state, action.item(), reward)\n",
        "          self.__replay_memory.push(agent_experience)\n",
        "          state = next_state\n",
        "          state_tensor = torch.tensor(state, device=self.__device).unsqueeze(0)\n",
        "\n",
        "        #store the expereince in replay memory\n",
        "        #time_stamp, state, action, next_state,\n",
        "\n",
        "        if time_step % self.__optimise_model_freq == 0:\n",
        "          print(f\"Model Optimise check started... ***************************************************************************\")\n",
        "          sample_batch = self.__sample_minibatch()\n",
        "          if sample_batch is not None:\n",
        "            print(f\"Optimisation check Approved ..\")\n",
        "            model_optimise_counter += 1\n",
        "            state_batch,next_state_batch, reward_batch, action_batch, non_final_state_mask, non_final_states_batch = self.__process_minibatch(sample_batch)\n",
        "            print(f\"Model Train - Iteration : {model_optimise_counter} - ****************************************************\")\n",
        "            self.__train_model(state_batch, reward_batch, action_batch, next_state_batch, non_final_state_mask)\n",
        "          else:\n",
        "            print(f\"Not enough data to optimise the model ..\")\n",
        "\n",
        "\n",
        "        #evaluate the agent use the reward list and calculate mean reward over a time;\n",
        "    if episode % self.__evaluate_model_freq == 0:\n",
        "      model_evaluate_counter += 1\n",
        "      #run the method for the current policy to play 100 episodes and take the mean reward\n",
        "      evaluate_mean_reward = self.__evaluate_agent(model_evaluate_counter)\n",
        "      if len(mean_evaluate_reward_list) == 0:\n",
        "        current_max = 0\n",
        "      else:\n",
        "        current_max = max(mean_evaluate_reward_list)\n",
        "      mean_evaluate_reward_list.append(evaluate_mean_reward)\n",
        "      #use a variable for best policy and best mean reward in evaluation process\n",
        "      if evaluate_mean_reward > current_max:\n",
        "        #save the policy as the current best policy\n",
        "        self.__current_best_policy = self.__online_network.state_dict()\n",
        "\n",
        "    if episode % self.__update_target_network_freq == 0:\n",
        "      # copy current online network parameters to target network (Hard Update)\n",
        "      self.__target_network.load_state_dict(self.__online_network.state_dict())\n",
        "\n",
        "    print(f\"Epsiode Number - {episode}, Total reward - {episode_reward}, Duration - {episode_steps}\")\n",
        "\n",
        "\n",
        "  def test_agent(self):\n",
        "    pass\n",
        "\n",
        "  def __evaluate_agent(self, iteration: int):\n",
        "    #use the currrent online network and let the agent play 100 episodes\n",
        "    #calculate the mean reward\n",
        "    self.__online_network.load_state_dict(self.__target_network.state_dict())\n",
        "    max_episodes = self.__model_evaluate_duration\n",
        "    eps_reward_array = np.empty(max_episodes)\n",
        "    mean_reward = 0\n",
        "    total_reward = 0\n",
        "    max_reward = 0\n",
        "    min_reward = 0\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "      episode_reward = 0\n",
        "      state, _ = self.env.reset()\n",
        "      state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "      for step_t in count():\n",
        "        action = self.__online_network(state_tensor).max(1).view(1,1)\n",
        "        next_state, reward, termi, _ = self.env.step(action.item())\n",
        "\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if termi:\n",
        "          break\n",
        "        else:\n",
        "          state = next_state\n",
        "          state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "      eps_reward_array[episode] = episode_reward\n",
        "    mean_reward = np.mean(eps_reward_array)\n",
        "    total_reward = np.sum(eps_reward_array)\n",
        "    max_reward = np.max(eps_reward_array)\n",
        "    min_reward = np.min(eps_reward_array)\n",
        "    print(f\" Agent Evaluate - Iteration :: {iteration}  --------------\")\n",
        "    print(f\" ---------------------------------------------------------\")\n",
        "    print(f\" Maximum reward recorded :: {max_reward}                  \")\n",
        "    print(f\" Minimum reward recorded :: {min_reward}                  \")\n",
        "    print(f\" Mean reward over {max_episodes} :: {mean_reward} --------\")\n",
        "    print(f\" ---------------------------------------------------------\")\n",
        "    return mean_reward\n",
        "\n",
        "\n",
        "  def get_train_performance(self) -> Tuple[list, list]:\n",
        "    perform_metrics = Tuple(self.__moving_average_reward_list, self.__episode_reward_list)\n",
        "    return perform_metrics\n",
        "\n",
        "  def get_test_performance(self) -> Tuple[list, list]:\n",
        "    pass\n",
        "\n",
        "\n",
        "  def __train_model(self, state_batch: torch.tensor, reward_batch: torch.tensor, action_batch: torch.tensor,\n",
        "                    next_state_batch: torch.tensor, non_final_state_mask: torch.tensor):\n",
        "    \"\"\"\n",
        "    1.Use our DQN network to select the best action to take for the next state\n",
        "      (the action with the highest Q-value).\n",
        "    2.Use our Target network to calculate the target Q-value of taking that action at the next state.\n",
        "    \"\"\"\n",
        "    #run the preporcessed data for one forward pass\n",
        "    non_final_next_state_batch = next_state_batch[non_final_state_mask]\n",
        "    print(f\"Shape : {non_final_next_state_batch.shape}\")\n",
        "    current_state_action_values = self.__online_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
        "    next_state_action_values = torch.zeros(self.__batch_size, device=self.__device)\n",
        "    #online_q_values.gather(1, action_batch.unsqueeze(1))\n",
        "    with torch.no_grad():\n",
        "      online_q_values = self.__online_network(non_final_next_state_batch)\n",
        "      best_online_actions = torch.argmax(online_q_values,1)\n",
        "      target_q_values = self.__target_network(non_final_next_state_batch)\n",
        "      next_state_action_values[non_final_state_mask] = target_q_values.gather(1, best_online_actions.unsqueeze(1))\n",
        "\n",
        "    expected_state_action_values = (next_state_action_values * self.__discount_factor) + reward_batch.unsqueeze(1)\n",
        "\n",
        "    criterion = nn.HuberLoss()\n",
        "    loss = criterion(current_state_action_values, expected_state_action_values)\n",
        "\n",
        "    self.__optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "   # torch.nn.utils.clip_grad_value_(self.__online_network.parameters(), 100)\n",
        "    self.__optimizer.step()\n",
        "\n",
        "\n",
        "  def __process_minibatch(self, batch: np.ndarray):\n",
        "    #(state, next_state, action, reward)\n",
        "    self.__data_processor.batch_to_processe(batch=batch)\n",
        "    state_array = self.__data_processor.filter_to_array(0)\n",
        "    nextstate_array = self.__data_processor.filter_to_array(1)\n",
        "    action_array = self.__data_processor.filter_to_array(2)\n",
        "    reward_array = self.__data_processor.filter_to_array(3)\n",
        "\n",
        "    non_final_masks = self.__data_processor.create_masks(check_array=nextstate_array, check_value= -1, is_final=False)\n",
        "\n",
        "    state_batch = torch.tensor(state_array, dtype=torch.float32, device=self.__device)\n",
        "    nextstate_batch = torch.tensor(nextstate_array, dtype=torch.float32, device=self.__device)\n",
        "    action_batch = torch.tensor(action_array, device=self.__device, dtype=torch.long)\n",
        "    reward_batch = torch.tensor(reward_array, device=self.__device)\n",
        "\n",
        "    mask_tensor = torch.tensor(non_final_masks, dtype=torch.bool, device=self.__device)\n",
        "    non_final_states_batch = nextstate_batch[mask_tensor]\n",
        "  #state_batch,next_state_batch, reward_batch, action_batch, non_final_state_mask, non_final_states_batch\n",
        "    return state_batch, nextstate_batch, reward_batch ,action_batch , mask_tensor, non_final_states_batch\n",
        "\n",
        "\n",
        "  def __sample_minibatch(self) -> np.ndarray:\n",
        "\n",
        "    if len(self.__replay_memory) < self.__batch_size:\n",
        "      return None\n",
        "    self.__sampler.get_expereinces(self.__replay_memory)\n",
        "    self.__sampler.create_valid_buffer()\n",
        "    self.__sampler.fill_buffer()\n",
        "    minibatch = self.__sampler.sample_batch()\n",
        "    return minibatch\n",
        "\n",
        "\n",
        "  def __select_action(self, epsilon: float, state: torch.tensor) -> torch.tensor:\n",
        "    prob = np.random.random()\n",
        "    if prob < epsilon:\n",
        "      return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        return self.__online_network(state).max(1)[0].view(1,1).detach()\n",
        "\n",
        "  def __shrink_epsilon(self, min_eps: float, max_eps: float, decay_rate: float, episode: int) -> float:\n",
        "    epsilon = min_eps + (max_eps - min_eps)*np.exp(-decay_rate*episode)\n",
        "    return epsilon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create hyper_parameters for DoublDQNAgent\n",
        "\n",
        "hyper_params = {\n",
        "  \"hidden_units\": 128,\n",
        "  \"discount_factor\": 0.99,\n",
        "  \"learning_rate\": 0.0001,\n",
        "  \"training_epochs\": 1000,\n",
        "  \"memory_capacity\": 10000,\n",
        "  \"epsilon_max\": 1.0,\n",
        "  \"epsilon_decay\": 0.995,\n",
        "  \"epsilon_min\": 0.01,\n",
        "  \"batch_size\": 32,\n",
        "  \"update_target_network_freq\": 100,\n",
        "  \"evaluate_model_freq\": 100,\n",
        "  \"optimise_model_freq\": 4,\n",
        "  \"model_evaluate_duration\": 100,\n",
        "  \"loss_function\": \"SmoothL1Loss\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "dtPkqKHRm77m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DoubleDQNAgent(hyper_params=hyper_params, env_name=\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "6gvbdtkonHg6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "EQZTDr8kp-hR",
        "outputId": "9ec67d11-98a9-4146-9e1e-97b0641cf3c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Episode No : 0 | Epsilon : 1.0 ******************************************************************\n",
            "Model Optimise check started... ***************************************************************************\n",
            "Not enough data to optimise the model ..\n",
            "Model Optimise check started... ***************************************************************************\n",
            "Not enough data to optimise the model ..\n",
            "Model Optimise check started... ***************************************************************************\n",
            "Not enough data to optimise the model ..\n",
            "Model Optimise check started... ***************************************************************************\n",
            "Not enough data to optimise the model ..\n",
            "Starting Episode No : 1 | Epsilon : 0.3760262100986184 ******************************************************************\n",
            "Model Optimise check started... ***************************************************************************\n",
            "Not enough data to optimise the model ..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "-0.030892275273799896 (<class 'float'>) invalid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bbb5886c5c60>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-3bcee60bfebd>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mepisode_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__select_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         assert self.action_space.contains(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         ), f\"{action!r} ({type(action)}) invalid\"\n",
            "\u001b[0;31mAssertionError\u001b[0m: -0.030892275273799896 (<class 'float'>) invalid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNoGRnzlp-8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiFzgw_xkfzT"
      },
      "outputs": [],
      "source": [
        "def plot_epsilon(num_eps, max_eps, min_eps, decay_rate):\n",
        "  x = np.arange(num_eps)\n",
        "  y = min_eps + (max_eps - min_eps)*np.exp(-decay_rate*x)\n",
        "  plt.plot(x, y, color='b', label='Epsilon Values')\n",
        "  plt.plot(x[-1],y[-1],marker=\"*\", markersize=4, color='red', label='End')\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Epsilon')\n",
        "  plt.title('Epsilon Convergence')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K45EHPLbuWAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3726ec55-8674-4226-a202-76171e79b196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16017034530570884\n"
          ]
        }
      ],
      "source": [
        "y = 0.2 + (0.2 - 1.0)*np.exp(-0.001*3000)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_epsilon(1000, 1.0,0.2,0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9v791X87mNBv",
        "outputId": "71db789b-71f3-48be-814c-205633648bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertion test for Aegent Experience numpy ndarray into tensor batches\n"
      ],
      "metadata": {
        "id": "_WoDkoIuCUtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_minibatch( exp_one:ReplayBuffer) -> np.ndarray:\n",
        "  minibatch = exp_one.sample(batch_size=32)\n",
        "  return minibatch\n"
      ],
      "metadata": {
        "id": "KHVHrQpH9IUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_to_array(batch: np.ndarray, filter_idx: int) -> np.ndarray:\n",
        "  filter_array = []\n",
        "  for i in range(len(batch)):\n",
        "    if batch[i] != 0:\n",
        "      filter_array.append(batch[i][filter_idx])\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  filter_array = np.array(filter_array)\n",
        "  return filter_array"
      ],
      "metadata": {
        "id": "LUpD0Ad7_LR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_array(filter_array: np.ndarray) -> np.ndarray:\n",
        "  clean_list = []\n",
        "\n",
        "  for i, data in enumerate(filter_array):\n",
        "    if isinstance(data, tuple):\n",
        "      clean_list.append(data[0])\n",
        "    else:\n",
        "      clean_list.append(data)\n",
        "  clean_array = np.array(clean_list)\n",
        "  return clean_array\n"
      ],
      "metadata": {
        "id": "rEZSxfT8EdRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sample(batch: np.ndarray):\n",
        "  state_filter = filter_to_array(batch, 1)\n",
        "  next_state_filter = filter_to_array(batch, 3)\n",
        "  reward_filter = filter_to_array(batch, 4)\n",
        "  action_filter = filter_to_array(batch, 2)\n",
        "\n",
        "  cleaned_state = clean_array(state_filter)\n",
        "  cleaned_next_state = clean_array(next_state_filter)\n",
        "  cleaned_reward = clean_array(reward_filter)\n",
        "  cleaned_action = clean_array(action_filter)\n",
        "\n",
        "  state_batch = torch.tensor(cleaned_state)\n",
        "  final_next_state_batch = torch.tensor([s for s in cleaned_next_state if s is None])\n",
        "  next_state_batch = torch.tensor([s for s in cleaned_next_state if s is not None])\n",
        "  reward_batch = torch.tensor(cleaned_reward)\n",
        "  action_batch = torch.tensor(cleaned_action)\n",
        "  non_final_mask = torch.tensor([s is not None for s in next_state_batch], dtype=torch.bool)\n",
        "  non_final_states_batch = torch.tensor([s for s in cleaned_next_state if s is not None])\n",
        "\n",
        "  return state_batch, next_state_batch, reward_batch, action_batch, non_final_mask, non_final_states_batch , final_next_state_batch"
      ],
      "metadata": {
        "id": "r4-NeO4uKqY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#agent_experience = (time_step, state, action, next_state, reward)\n",
        "#my_env = DiscreteEnvironment()\n",
        "#my_env.build_environment(\"CartPole-v1\")\n",
        "#env = gym.make(\"CartPole-v1\")\n",
        "#rp = ReplayBuffer(max_size=1000)\n",
        "import gym\n",
        "import time\n",
        " # Replace 'your_module' with the actual module name where the ReplayBuffer class is defined\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"LunarLander-v2\", new_step_api=True)\n",
        "\n",
        "# Initialize a ReplayBuffer\n",
        "rp = ReplayBuffer(max_size=10000)  # You need to define and instantiate ReplayBuffer class\n",
        "\n",
        "# Reset the environment to get the initial observation\n",
        "#observation = env.reset()\n",
        "#state = observation[0]  # Assuming the initial state is the observation itself\n",
        "stepcount = 0\n",
        "# Run the loop for a certain number of steps (200 in this case)\n",
        "for e in range(100):\n",
        "  observation = env.reset()\n",
        "  state = observation[0]\n",
        "  for _ in count():\n",
        "      # Take a random action\n",
        "      stepcount += 1\n",
        "      action = env.action_space.sample()\n",
        "      print(\"Action taken:\", action)\n",
        "\n",
        "      # Perform the action in the environment and get the next state, reward, done, and info\n",
        "      next_observation = env.step(action)\n",
        "      next_state = next_observation[0]\n",
        "      reward = next_observation[1]\n",
        "      done = next_observation[2]\n",
        "      # Store the experience tuple in the replay buffer\n",
        "      print(f\"{stepcount, state, action, next_state, reward}\")\n",
        "      time.sleep(0.001)\n",
        "      agent_experience = (stepcount, state, action, next_state,reward)\n",
        "      rp.push(agent_experience)\n",
        "\n",
        "      # Update the current state\n",
        "      state = next_state\n",
        "\n",
        "      # If the episode is done (in our case we land, crash, or timeout)\n",
        "      if done:\n",
        "          # Reset the environment to start a new episode\n",
        "          print(\"Environment is reset\")\n",
        "          #observation = env.reset()\n",
        "          state = observation\n",
        "          break  # Update the state to the new observation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5TL8hgk4w_T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rp)"
      ],
      "metadata": {
        "id": "LgSmnRsJGZCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam = sample_minibatch(rp)"
      ],
      "metadata": {
        "id": "EORXk2Q5HToF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sam)"
      ],
      "metadata": {
        "id": "229_cDeZR48N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\", new_step_api=True)\n",
        "obs = env.reset()\n",
        "o = env.step(env.action_space.sample())"
      ],
      "metadata": {
        "id": "VQSR-u9BDuex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(o)\n",
        "s = o[0]\n",
        "r =o[1]\n",
        "ter = o[2]\n",
        "tru =o[3]"
      ],
      "metadata": {
        "id": "a9qb8IS26wZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Type of state_batch: {type(a)}, Shape of state_batch: {a.shape}\")\n",
        "print(f\"Type of next_state_batch: {type(b)}, Shape of next_state_batch: {b.shape}\")\n",
        "print(f\"Type of reward_batch: {type(c)}, Shape of reward_batch: {c.shape}\")\n",
        "print(f\"Type of action_batch: {type(d)}, Shape of action_batch: {d.shape}\")\n",
        "print(f\"Type of non_final_mask: {type(e)}, Shape of non_final_mask: {e.shape}\")\n",
        "print(f\"Type of non_final_states_batch: {type(f)}, Shape of non_final_states_batch: {f.shape}\")"
      ],
      "metadata": {
        "id": "09NgcY7I7c2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}